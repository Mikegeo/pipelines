#!/bin/bash
#PBS -l nodes=1:ppn=32,walltime=100:00:00,vmem=60g,mem=220g
#PBS -N variantCalls
#PBS -q himem
#PBS -V
#PBS -j oe
#PBS -m abe
#PBS -M slei.bass@gmail.com

:'
	This batch script accomplishes 6 Layers of protocols:
	- Task 1: Index merged bam files
	  + Exome QC (hg19)
	- Task 2: Call copy number variants single samples at a time
	  + Copy number calling (CNVs)
	- Task 3: Call, categorize, filter clean point mutations
	  + Variant calling (SNVs, indels) and quality recalibration (Bcftools, Varscan2, GATK)
	- Task 4: Annotate mutations
	  + Variant annotation (ClinVar, dbSNP, Exac, Cosmic)
	- Task 5: Data mining (VCF formatting, thresholding, feature wrangling)
	  + Data reformatting for pattern recognition procedures
	  + Variant multi-selection based on scaled-down filters
	  + Variant thresholding (mutli-metrics, function)
	  + Data mining and multi-metric summary reporting
	- Task 6: Clonal evolution from same individual dual exome samples
	  + Clonal evolution (pyclone)
'

set -eux

module load java/8 samtools/1.9 bedtools/2.27.1
module load HTSeq/0.7.2 bwa/0.7.15 bedops/2.4.14 picard/2.10.9 
module load bamtools/2.4.2 tabix/0.2.6 varscan/2.4.2 
module load snpEff/4.3
module load R/3.5.0 python/2.7 
module load gatk/3.8
#module load python3/3.6.5 CNVkit/0.9.3

## use the below exec when loading R
## java -jar $gatk_dir/gatk-package-4.0.5.1-local.jar HaplotypeCaller


## H4H directories
## Misc tools parameters
scratch=/cluster/projects/kridelgroup
_storage=$scratch/relapse/mutations
home=/cluster/home/sbassim
user_databases=$scratch/databases
admin_databases=/cluster/tools/data/genomes/human/hg19/variantcallingdata
pbs=$(echo $PBS_JOBID | cut -f1 -d '.')
time=$home/time
jobid=$pbs.exome_calling.multitool.annotated_pyclone
start=$(date); echo "Job started at: $start" > $time/$jobid.time

# __CHANGE__
_realign2indels=FALSE
_call2variants=TRUE
cnvs=cnmops
pipeline=varscan2
_protocol=removeFunctionV3
normal_cnv=2
minor_cnv=0
_priors=major_copy_number
_read_depth=10
_read_depth_bcftools=2
_read_depth_gatk=20
_read_phred=30
_allel_pval=10e-10
_allel_freq=5
_call_quality=5
_pyclone_method=pyclone_binomial

# __DONT CHANGE__
compression=0
threads=32
mapq=39
_gVMEM=50g
snpeff_genome=GRCh37.75
_data=$_storage/raw
workdir=$_storage/pyclone_analysis/$pbs
snpeff_config=$home/snpEff/snpEff.config
agilent=$_storage/targets/padded.clean.bed
ucsc=$_storage/targets/exons.ucsc.clean.bed
_pyclone_config=$_storage/pyclone_configs
## idx created from ClinVar annotating current samples
_variant_function=$_storage/variant_function.idx
_EXAC=$user_databases/gnomad.rs_position.idx
_COSMIC=$user_databases/cosmic/CosmicCodingMuts.vcf.gz
_COSMIC_mut=$user_databases/cosmic/CosmicMutantExportCensus.tsv.gz
genome_reference=$user_databases/GRCh37-lite.fa
## make sure chromosomes in gold_indels match bam (eg, chr1 or 1)
gold_indels=$user_databases/Mills.gold.standard.adapted_noChr.vcf

mkdir -p $workdir/yaml/$_priors


###########################
# Preprocessing BAM reads #
###########################
## index bam samples after merging
## required for samtools and gatk workflows
cd $_data
_listingRaw=$(find $_data -name "*Flagged.bam")

if [ $_realign2indels == "TRUE" ]; then
    for _raw in $_listingRaw; do

	if [ ! -e $_raw.bai ]; then
	    ## index files
	    samtools index -@$threads $_raw
	fi

	## realign reads around indels
	sample_label=$(basename $_raw | cut -f1 -d'_')
	java -Xmx$_gVMEM -jar $gatk_dir/GenomeAnalysisTK.jar -T RealignerTargetCreator \
	     -R $genome_reference \
	     -I $_raw \
	     -known $gold_indels \
	     -o $sample_label.intervals

	java -Xmx$_gVMEM -jar $gatk_dir/GenomeAnalysisTK.jar -T IndelRealigner \
	     -R $genome_reference \
	     -I $_raw \
	     -known $gold_indels \
	     -targetIntervals $sample_label.intervals \
	     -o $sample_label.realigned.bam
    done
fi

#############################################
# Indexing and calling copy number variants #
# Batch sample CNV calling		    #
#############################################
## get the total number of copy variants per sample
cd $workdir

if [ $_call2variants == "TRUE" ]; then
    if [ $cnvs == "cnvkit" ]; then
	cnvkit.py batch *.bam -n \
		  -t $agilent \
		  -f $genome_reference \
		  --short-names \
		  --output-reference $workdir/cnvkit.index_reference.cnn \
		  --annotate $ucsc \
		  -d $workdir/output.2_bash.$cnvs.multi_samples.cnv.txt

    elif [ $cnvs == "cnmops" ]; then
	R CMD BATCH $home/script*/r/cnv.detection.R

    else
	echo "Copy number calling approach not specified!"
	exit 0
    fi
fi


##############################################
# Single polymorphism on individual samples  #
# No batch sampling (because pyclone)	     #
# Mutation annotation with ClinVar and dbSnp #
##############################################
## run analysis one sample at a time
## the following will create one annotated pileup VCF per sample
## one VCF per sample ensures correct execution of pyClone
cd $workdir
_listingFiles=$(find $_data -name "*realigned.bam")
for _idx in $_listingFiles; do

    sample_label=$(basename $_idx | cut -f1 -d'.')
    tumor_label=$(basename $_idx | cut -f2 -d'.')
    _DIR=$(dirname $_idx)

    output[1]=output.1_bash.$pipeline.samtools.mpileup.agilent.$sample_label

    if [ $_call2variants == "TRUE" ]; then
	if [ $pipeline == "varscan2" ]; then
	    module load samtools/1.3.1
	    samtools mpileup -EC 50 --ignore-RG -l $agilent -f $genome_reference $_idx \
		| java -jar $varscan_dir/VarScan.jar mpileup2snp - \
		       --min-var-freq 0.01 \
		       --p-value 0.01 \
		       --output-vcf 1 \
		       > ${output[1]}.vcf

	elif [ $pipeline == "bcftools" ]; then
	    ## generate multi-way pileup genotype likelihoods
	    ## max-depth is in effect when the -d 8000 default is passed
	    ## multiple samples can be used, pyclone requires one at a time
	    ## bcftools mpileup can be piped for calls for all samples
	    ## call snps using bcftools multiallelic -m
	    ## discard indel calls
	    module load samtools/1.9
	    bamtools filter -tag XT:U -in $_idx \
		| samtools view -u -b - -@$threads \
		| bcftools mpileup -Ou -EC 40 -d 8000 \
			   --annotate AD,ADF,ADR \
			   --targets-file $agilent \
			   --ignore-RG -f $genome_reference - \
		| bcftools call -vmO z --skip-variants indels -o ${output[1]}.vcf.gz

	    sleep 30

	    ## samtools mpileup stats
	    tabix -p vcf ${output[1]}.vcf.gz
	    bcftools stats -F $genome_reference -s - ${output[1]}.vcf.gz > ${output[1]}.stats.txt

	    gzip -d ${output[1]}.vcf.gz

	elif [ $pipeline == "gatk" ]; then
	    module load gatk/4.0.5.1
	    java -Xmx$_gVMEM -jar $gatk_dir/gatk-package-4.0.5.1-local.jar Mutect2 \
		 -R $genome_reference \
		 -I $_idx \
		 -tumor $tumor_label \
		 --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter \
		 -O ${output[1]}.vcf.gz

	    sleep 30
	    gzip -d ${output[1]}.vcf.gz

	else
	    echo "SNP calling approach not specified!"
	    exit 0
	fi

	## get the total number of copy variants per sample
	output[2]=output.2_r.$cnvs.multi_samples.cnv
	total_cnv=$(cut -f6 ${output[2]}.txt | sed '1d' | sort - | uniq -c | awk -vc=$sample_label '{if($2 == c)print$1}')

    else

	if [ $pipeline == "varscan2" ]; then
	    cp $_storage/pyclone_analysis/516274/variants/output.1_bash*$sample_label.vcf.gz $workdir
	    cd $workdir
	    gzip -d output.1_bash*vcf.gz

	elif [ $pipeline == "bcftools" ]; then
	    cp $_storage/pyclone_analysis/517376/variants/output.1_bash*$sample_label.vcf.gz $workdir
	    cd $workdir
	    gzip -d output.1_bash*vcf.gz

	elif [ $pipeline == "gatk" ]; then
	    exit 0
	    
	else
	    echo "Error! Did not produce VCF files and did not import pre-existent ones."
	    exit 0
	fi
	
    fi

    ## Override the maximum number of copy number variants
    total_cnv=2

    ## CHOICE 1
    ## annotate variants with human genome GRCh37
    ## annotate variants with dbsnp
    ## annotate variants with clinvar
    ## optional parameters -a -v -d
    output[3]=output.3_bash.$pipeline.snpeff_annot.$snpeff_genome.clinvar.dbsnp.$sample_label
    java -jar $snpeff_dir/SnpSift.jar annotate \
	 -noLog -noDownload \
	 -c $snpeff_config \
	 -clinvar \
	 ${output[1]}.vcf \
	| java -jar $snpeff_dir/SnpSift.jar annotate \
	       -noLog -noDownload \
	       -c $snpeff_config \
	       -dbsnp - \
	| java -jar $snpeff_dir/snpEff.jar eff \
	       -noDownload -noLog \
	       -c $snpeff_config \
	       -stats ${output[3]}.html \
	       $snpeff_genome \
	       > ${output[3]}.vcf

    output[4]=output.4_bash.$pipeline.mining.snpeff_annot.$snpeff_genome.clinvar.dbsnp.$sample_label
    # ## create personalized input for pyclone without annoated VCF
    # grep -v "^#" ${output[1]}.vcf \
    # 	| cut -f1-2,4-5,10 \
    # 	| sed 's/:/\t/g' \
    # 	| awk -F"\t" '{print$1":"$2,$9,$10}' \
    # 	| sed -e 's/ /\t/g' -e "s/$/\\t$normal_cnv\\t$minor_cnv\\t$total_cnv/g" \
    # 	      > ${output[4]}.tsv
    # sed -i "1i$(echo -e "mutation_id\tref_counts\tvar_counts\tnormal_cn\tminor_cn\tmajor_cn")" ${output[4]}.tsv


    output[5]=output.5_bash.$pipeline.snpeff_annot.dbsnp.$sample_label
    # ## choice 2
    # ## annotate only with dbSnp
    # java -jar $snpeff_dir/SnpSift.jar annotate \
    # 	 -noLog -noDownload \
    # 	 -c $snpeff_config \
    # 	 -dbsnp \
    # 	 ${output[1]}.vcf \
    # 	 > ${output[5]}.vcf

    sleep 60
    gzip ${output[1]}.vcf
    sleep 30

    output[6]=output.6_bash.$pipeline.snpeff_annot.$_protocol.noFilter_clean.$sample_label
    output[7]=output.7_bash.$pipeline.snpeff_annot.$_protocol.reduced.clean_pyclone.$sample_label
    ## create personalized input for pyclone from annotated VCF
    ## get annotated (rs*) mutations (MU)
    ## clean based on annotated MU (dbsnp)
    ## clean double annotated MU (rs;rs)
    ## clean based on ADP (avg depth coverage phred >= 15)
    ## clean based on allele frequency
    ## clean based on allele p-value
    ## format table to include depth -i to reference and ii- to variant
    ## clean rows with false MU index

    if [ $pipeline == "varscan2" ]; then

	if [ $_protocol == "keepAnnotated" ]; then

	    ## based on dbSnp annotation formatting
	    ## this will keep gene names and rs ID
	    grep -v "^#" ${output[5]}.vcf \
		| awk 'match($3, /rs.*/) {print $0}' \
		| sed -e 's/WT=.*GENEINFO=//g' -e 's/:.*ADR\t/\t/g' -e 's/PASS.*ADP=//g' \
		| grep -v "=" \
		| sed -e 's/\trs.*;rs/\trs/g' -e 's/:/\t/g' -e 's/;/\t/g' -e 's/%//g' \
		| tee ${output[6]}.txt \
		| awk -vd=$_read_depth '{if($7 >= d) print $0}' \
		| awk -vp=$_allel_pval '{if($16 <= p) print $0}' \
		| awk -va=$_allel_freq '{if($15 <= a) print $0}' \
		| awk -F"\t" '{print$8"_"$1":"$2,$13,$14}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		| grep -v "^\." \
		| grep -wv "FREQ" - \
		       > ${output[7]}.tsv

	elif [ $_protocol == "removeAnnotated" ]; then

	    ## based on dbSnp annotation formatting
	    ## this wont keep gene names
	    grep -v "^#" ${output[5]}.vcf \
		| grep -v "rs" \
		| cut -f1-5,10 \
		| sed -e 's/:/\t/g' -e 's/%//g' \
		| tee ${output[6]}.txt \
		| awk -vp=$_allel_pval '{if($13 <= p) print $0}' \
		| awk -va=$_allel_freq '{if($12 >= a) print $0}' \
		| awk -F"\t" '{print$1":"$2,$10,$11}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		| grep -v "^\." \
		| grep -wv "FREQ" - \
		       > ${output[7]}.tsv

	elif [ $_protocol == "keepClinvar" ]; then
	    
	    ## output formatting based on clinvar and dbSnp annotation
	    ## this will keep gene handles
	    grep -v "^#" ${output[3]}.vcf \
		| grep -v "rs" \
		| sed -e 's/;WT=.*ANN=.|/\t/g' -e 's/|ENS.*:ADR//g' \
		| cut -f1-2,4-5,8-10 \
		| sed -e 's/ADP=//g' -e 's/|/\t/g' -e 's/:/\t/g' -e 's/%//g' \
		| tee ${output[6]}.txt \
		| awk -vd=$_read_depth '{if($5 >= d) print $0}' \
		| awk -vp=$_allel_pval '{if($16 <= p) print $0}' \
		| awk -va=$_allel_freq '{if($15 >= a) print $0}' \
		| awk -F"\t" '{print$8"_"$1":"$2,$13,$14}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		| grep -v "^\." \
		| grep -wv "FREQ" - \
		       > ${output[7]}.tsv


	elif [ $_protocol == "removeFunctionV1" ]; then
	    
	    ## output formatting based on clinvar and dbSnp annotation
	    ## this will keep gene handles
	    ## this will discard variants based on their functions (stop ..)
	    ## uses allele pvalue to select high confidence variants
	    grep -v "^#" ${output[3]}.vcf \
		| grep -v "rs" \
		| grep -iv "^[a-z]" \
		| sed -e 's/;WT=.*ANN=.|/\t/g' -e 's/|ENS.*:ADR//g' \
		| cut -f1-2,4-5,8-10 \
		| sed -e 's/ADP=//g' -e 's/|/\t/g' -e 's/:/\t/g' -e 's/%//g' \
		| tee ${output[6]}.txt \
		| grep -vFwf <(cat $_variant_function) - \
		| awk -vp=$_allel_pval '{if($16 <= p) print $0}' \
		| awk -F"\t" '{print$8"_"$1":"$2,$13,$14}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		| grep -v "^\." \
		| grep -wv "FREQ" - \
		       > ${output[7]}.tsv

	elif [ $_protocol == "removeFunctionV2" ]; then
	    
	    ## output formatting based on clinvar and dbSnp annotation
	    ## this will keep gene handles
	    ## this will discard variants based on their functions (stop ..)
	    ## uses variant depth, allele freq, and variant/reference ratio to discard SNVs
	    grep -v "^#" ${output[3]}.vcf \
		| grep -v "rs" \
		| grep -iv "^[a-z]" \
		| sed -e 's/;WT=.*ANN=.|/\t/g' -e 's/|ENS.*:ADR//g' \
		| cut -f1-2,4-5,8-10 \
		| sed -e 's/ADP=//g' -e 's/|/\t/g' -e 's/:/\t/g' -e 's/%//g' \
		| tee ${output[6]}.txt \
		| grep -vFwf <(cat $_variant_function) - \
		| awk -vd=$_read_depth '{if($5 >= d) print $0}' \
		| awk -vp=$_allel_pval '{if($16 <= p) print $0}' \
		| awk -F"\t" '{print$8"_"$1":"$2,$13,$14}' \
		| awk '{if($2 > $3) print $0}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		| grep -v "^\." \
		| grep -wv "FREQ" - \
		       > ${output[7]}.tsv

	elif [ $_protocol == "removeFunctionV3" ]; then
	    
	    ## output formatting based on clinvar and dbSnp annotation
	    ## this will keep gene handles
	    ## this will discard variants based on their functions (stop ..)
	    ## uses variant depth, allele freq, and pvalues
	    ## this will discard variants also found in exAC
	    grep -v "^#" ${output[3]}.vcf \
		| grep -v "rs" \
		| grep -iv "^[a-z]" \
		| sed -e 's/;WT=.*ANN=.|/\t/g' -e 's/|ENS.*:ADR//g' \
		| cut -f1-2,4-5,8-10 \
		| sed -e 's/ADP=//g' -e 's/|/\t/g' -e 's/:/\t/g' -e 's/%//g' \
		| grep -vFwf <(cat $_EXAC) \
		| tee ${output[6]}.txt \
		| grep -vFwf <(cat $_variant_function) - \
		| awk -vd=$_read_depth '{if($5 >= d) print $0}' \
		| awk -va=$_allel_freq '{if($15 > a) print $0}' \
		| egrep -iv "^\.|muc|usp|rp11" \
		| awk -F"\t" '{print$8"_"$1":"$2,$13,$14}' \
		| sed -e 's/ /\t/g' -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" \
		       > ${output[7]}.tsv

	else


	    echo "Must specify whether to remove or keep annotated known SNVs!"
	    exit 0

	fi
	
    elif [ $pipeline == "bcftools" ]; then

	## output formatting based on clinvar and dbSnp annotation
	## this will keep gene handles
	## this will discard variants based on their functions (stop ..)
	## this will discard variants also found in exAC
	## Keep only new sites (ID column must be ".")
	grep -v "^#" ${output[3]}.vcf \
	    | grep -v "rs" \
	    | grep -iv "^[a-z]" \
	    | awk 'match($3, /\./) {print $0}' \
	    | grep -vFwf <(cat $_EXAC) \
	    | sed -e 's/;/\t/g' -e 's/\tDP=/\t/g' -e 's/SGB=.*MQ=/MQ=/g' -e 's/VDB=.*MQ=/MQ=/g' \
	    | sed -e 's/MQ=//g' -e 's/ANN=.|//g' -e 's/|ENS.*AD//g' -e 's/|/\t/g' \
	    | cut -f1,5 -d':' \
	    | sed -e 's/\t.\/.*:/\t/g' -e 's/,/\t/g' \
	    | grep -v ">" \
	    | tee ${output[6]}.txt \
	    | grep -vFwf <(cat $_variant_function) - \
	    | egrep -iv "^\.|muc|usp|rp11" \
	    | awk -F"\t" '{print$12"_"$1":"$2,$13,$14}' \
	    | sed -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" -e 's/ /\t/g' \
		  > ${output[7]}.tsv

##	    | awk -vql=$_call_quality '{if($6 >= ql) print $0}' \
##	    | awk -vdp=$_read_depth_bcftools '{if($8 >= dp) print $0}' \
##	    | awk -vph=$_read_phred '{if($9 >= ph) print $0}' \

    elif [ $pipeline == "gatk" ]; then

	## output formatting based on clinvar and dbSnp annotation
	## this will keep gene handles
	## this will discard variants based on their functions (stop ..)
	## this will discard variants also found in exAC
	## Keep only new sites (ID column must be ".")
	## remove indels
	grep -v "^#" ${output[3]}.vcf \
	    | grep -v "rs" \
	    | grep -iv "^[a-z]" \
	    | awk 'match($3, /\./) {print $0}' \
	    | grep -vFwf <(cat $_EXAC) \
	    | sed -e 's/;/\t/g' -e 's/\tDP=/\t/g' -e 's/ECNT=.*ANN=/ANN=/g' \
	    | sed -e 's/ANN=[A-Z]*|//g' -e 's/|ENS.*PROB//g' \
	    | cut -f1,2 -d':' \
	    | sed -e 's/\t.\/.*:/\t/g' -e 's/,/\t/g' -e 's/|/\t/g' \
	    | awk '$4=="A" || $4=="G" || $4=="C" || $4=="T"' \
	    | awk '$5=="A" || $5=="G" || $5=="C" || $5=="T"' \
	    | tee ${output[6]}.txt \
	    | grep -vFwf <(cat $_variant_function) - \
	    | awk -vdp=$_read_depth_gatk '{if($8 >= dp) print $0}' \
	    | egrep -iv "^\.|muc|usp|rp11" \
	    | awk -F"\t" '{print$11"_"$1":"$2,$12,$13}' \
	    | sed -e "s/$/\t$normal_cnv\t$minor_cnv\t$total_cnv/g" -e 's/ /\t/g' \
		  > ${output[7]}.tsv

##	    | grep -vFwf <(zcat $_COSMIC | cut -f2 | sed '1d') - \
##	    | grep -vFwf <(zcat $_COSMIC_mut | grep -o ":.*-" | sed -e 's/://g' -e 's/-.*$//g') \

    else
	echo "VCF data reformatting for pattern recognition procedures not specified!"
	exit 0
    fi


    ## add headers
    sed -i "1i$(echo -e "mutation_id\tref_counts\tvar_counts\tnormal_cn\tminor_cn\tmajor_cn")" ${output[7]}.tsv




    ## summary reporting
    ## descriptive statistics
    ## distribution of variant by function
    if [ $pipeline == "varscan2" ]; then
	_fct_col=6
    elif [ $pipeline == "bcftools" ]; then
	_fct_col=10
    elif [ $pipeline == "gatk" ]; then
	_fct_col=9
    else
	echo "Error from wrong column assignment for SNV function extraction!"
	exit 0
    fi

    ## aggregate all selected metrics into one report
    output[8]=output.8_bash.summary.snv_functions.$pipeline.txt
    grep -v "^#" ${output[6]}.txt \
	| cut -f$_fct_col \
	| sort - \
	| uniq -c \
	| awk '{print $2,"\t",$1}' \
	| sed "s/$/\t$sample_label/g" \
	      >> ${output[8]}

    ## additional summary reporting
    if [ $pipeline == "varscan2" ]; then

	## pvalues and variant frequencies (VAF) by all SNV position
	output[9]=output.9_bash.summary.snv.preFilter_distributions.$pipeline.txt
	grep -v "^#" ${output[6]}.txt \
	    | awk -F"\t" '{print$8"_"$1":"$2,$5,$13,$14,$15,$16}' \
	    | sed "s/$/\t$sample_label/g" \
		  >> ${output[9]}

	## pvalues and variant allele frequencies (VAF) by SNV position
	## after discarding based on filters
	output[10]=output.10_bash.summary.snv.postFilter_VAFpval.$pipeline.txt
	grep -v "^#" ${output[6]}.txt \
	    | grep -vFwf <(cat $_variant_function) - \
	    | egrep -iv "^\.|muc|usp|rp11" \
	    | awk -vd=$_read_depth '{if($5 >= d) print $0}' \
	    | awk -va=$_allel_freq '{if($15 > a) print $0}' \
	    | awk -F"\t" '{print$8"_"$1":"$2,$5,$13,$14,$15,$16}' \
	    | sed "s/$/\t$sample_label/g" \
		  >> ${output[10]}

    fi

    sleep 60
    gzip ${output[3]}.vcf
    sleep 30


done

## add headers for summary reports
sed -i "1i $(echo -e "fct\tcount\tsample")" ${output[8]}
## additional summary reporting
## add headers for summary reports
if [ $pipeline == "varscan2" ]; then

    sed -i "1i $(echo -e "snv\tDP\trefCounts\tvarCounts\tvaf\tpval\tsample")" ${output[9]}
    sed -i "1i $(echo -e "snv\tDP\trefCounts\tvarCounts\tvaf\tpval\tsample")" ${output[10]}

fi


##############################################
# Clonal evolution by pair samples (pyClone) #
##############################################
## Below code works well
## But it should be deprecated
## That because using pyclone standard filters
## Cell prevalence is not well interepretable
## Please use the standalone pyclone script

## preprocess VCF for assessement of clonal evolution
## main tool: pyclone
cd $workdir

## create config files, manually
## only samples from same individual are compared together
_yaml=$(find $_pyclone_config -name "*.yaml")

## change the directory of each config file
## for each analysis using same samples but different iteration
for _pyCfg in $_yaml; do
    new_name=$(basename $_pyCfg)

    cat $_pyCfg \
	| sed '2d' \
	      > $workdir/$new_name
    sed -i "1i$(echo "density: $_pyclone_method")" $workdir/$new_name
    sed -i "1i$(echo "working_dir: $workdir")" $workdir/$new_name
        
done


## run python 2.7
## load pyclone 0.13.1 that I deployed
## do not load h4h pyclone (not correctly installed)
## TypeError: int() argument must be a string or a number, not 'NoneType'
## solved by replacing spaces with tabs in tsv output[7]
source activate pyclone

_listingFiles=$(find $_data -name "*realigned.bam")
for _idx in $_listingFiles; do

    sample_label=$(basename $_idx | cut -f1 -d'.')
    output[7]=output.7_bash.$pipeline.snpeff_annot.$_protocol.reduced.clean_pyclone.$sample_label
    ## build mutation file
    PyClone build_mutations_file \
	    --in_file ${output[7]}.tsv \
	    --out_file $workdir/yaml/$sample_label.yaml \
	    --prior $_priors

done


## run final clonal evolution analysis
## MCMC 10,000 iterations
_configFiles=$(find $workdir -name "config*yaml")
for _cfg in $_configFiles; do

    _ix=$(basename $_cfg | sed -e 's/config.//g' -e 's/.yaml//g')
    
    ## build configuration file (manually)
    PyClone run_analysis --config_file $_cfg

    ## summary table
    PyClone build_table --config_file $_cfg \
	    --out_file pyclone.$_ix.exome.summary_table.$pipeline.txt \
	    --table_type loci

    ## plot
    PyClone plot_loci --config_file $_cfg \
	    --plot_file pyclone.$_ix.parallel.$pipeline.png \
	    --plot_type parallel_coordinates
    PyClone plot_loci --config_file $_cfg \
	    --plot_file pyclone.$_ix.similarity_matrix.$pipeline.png \
	    --plot_type similarity_matrix
##    PyClone plot_loci --config_file $_cfg \
##	    --plot_file pyclone.$_ix.density.$pipeline.png \
##	    --plot_type density

done

source deactivate


## repo organization
sleep 300
cd $workdir
mkdir configs figures pyclone_input reports variants
mv *tsv pyclone_input
mv *gz* variants
mv *txt reports
mv *pdf *png *html figures
mv conf*yaml configs




end=$(date); echo "Job ended at: $end" >> $time/$jobid.time
